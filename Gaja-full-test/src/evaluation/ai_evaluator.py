"""Surowy system oceny AI z lokalnym modelem LM Studio."""

import json
import aiohttp
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum
from loguru import logger


class EvaluationCriteria(Enum):
    """Kryteria oceny odpowiedzi AI."""
    ACCURACY = "accuracy"  # Czy odpowiedÅº jest faktycznie poprawna
    RELEVANCE = "relevance"  # Czy odpowiedÅº jest na temat
    COMPLETENESS = "completeness"  # Czy odpowiedÅº jest kompletna
    TOOL_USAGE = "tool_usage"  # Czy uÅ¼yto odpowiednich narzÄ™dzi
    CONTEXT_UNDERSTANDING = "context_understanding"  # Czy zrozumiano kontekst
    PROACTIVITY = "proactivity"  # Czy AI dziaÅ‚aÅ‚o proaktywnie
    CONVERSATION_FLOW = "conversation_flow"  # Czy rozmowa pÅ‚ynie naturalnie
    ERROR_HANDLING = "error_handling"  # Czy bÅ‚Ä™dy sÄ… obsÅ‚ugiwane
    MEMORY_USAGE = "memory_usage"  # Czy uÅ¼ywa pamiÄ™ci kontekstowej
    PERSONALIZATION = "personalization"  # Czy odpowiedÅº jest spersonalizowana


@dataclass
class EvaluationResult:
    """Wynik oceny pojedynczego kryterium."""
    criteria: EvaluationCriteria
    score: float  # 0.0 - 10.0
    max_score: float  # Maksymalny moÅ¼liwy wynik
    reasoning: str  # Uzasadnienie oceny
    issues: List[str]  # Lista znalezionych problemÃ³w
    suggestions: List[str]  # Sugestie poprawy
    severity: str  # "low", "medium", "high", "critical"


@dataclass
class ConversationEvaluation:
    """PeÅ‚na ocena caÅ‚ej konwersacji."""
    conversation_id: str
    scenario_name: str
    total_score: float
    max_possible_score: float
    success_percentage: float
    criteria_results: List[EvaluationResult]
    overall_issues: List[str]
    overall_suggestions: List[str]
    conversation_analysis: str
    critical_failures: List[str]
    passes_quality_gate: bool


class AIEvaluator:
    """GÅ‚Ã³wny ewaluator uÅ¼ywajÄ…cy lokalnego modelu LM Studio."""
    
    def __init__(self, lm_studio_url: str = "http://127.0.0.1:1234", model_name: str = "openai/gpt-oss-20b"):
        self.lm_studio_url = lm_studio_url.rstrip('/')
        self.model_name = model_name
        self.quality_gate_threshold = 75.0  # Minimalna ocena Å¼eby przejÅ›Ä‡
        
    async def test_connection(self) -> bool:
        """Testuje poÅ‚Ä…czenie z LM Studio."""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.lm_studio_url}/v1/models") as response:
                    if response.status == 200:
                        data = await response.json()
                        models = [m["id"] for m in data.get("data", [])]
                        if self.model_name in models:
                            logger.info(f"âœ… PoÅ‚Ä…czono z LM Studio, model {self.model_name} dostÄ™pny")
                            return True
                        else:
                            logger.error(f"âŒ Model {self.model_name} niedostÄ™pny. DostÄ™pne: {models}")
                            return False
                    return False
        except Exception as e:
            logger.error(f"âŒ BÅ‚Ä…d poÅ‚Ä…czenia z LM Studio: {e}")
            return False
    
    async def evaluate_conversation(
        self, 
        conversation: List[Dict[str, Any]], 
        scenario_description: str,
        expected_behaviors: Optional[List[str]] = None
    ) -> ConversationEvaluation:
        """Ocenia caÅ‚Ä… konwersacjÄ™ wedÅ‚ug wszystkich kryteriÃ³w."""
        
        logger.info(f"ğŸ” Rozpoczynam BEZWZGLÄ˜DNÄ„ ocenÄ™ konwersacji...")
        
        # SprawdÅº poÅ‚Ä…czenie
        if not await self.test_connection():
            raise Exception("Nie moÅ¼na poÅ‚Ä…czyÄ‡ siÄ™ z LM Studio!")
        
        criteria_results = []
        total_score = 0.0
        max_possible_score = 0.0
        critical_failures = []
        
        # Ocena wedÅ‚ug kaÅ¼dego kryterium
        for criteria in EvaluationCriteria:
            result = await self._evaluate_single_criteria(
                conversation, 
                scenario_description, 
                criteria,
                expected_behaviors or []
            )
            criteria_results.append(result)
            total_score += result.score
            max_possible_score += result.max_score
            
            # Zbierz krytyczne bÅ‚Ä™dy
            if result.severity == "critical":
                critical_failures.extend(result.issues)
        
        # Analiza ogÃ³lna konwersacji
        conversation_analysis = await self._analyze_conversation_flow(conversation, scenario_description)
        
        # Zbierz wszystkie problemy i sugestie
        all_issues = []
        all_suggestions = []
        for result in criteria_results:
            all_issues.extend(result.issues)
            all_suggestions.extend(result.suggestions)
        
        success_percentage = (total_score / max_possible_score * 100) if max_possible_score > 0 else 0
        passes_quality_gate = success_percentage >= self.quality_gate_threshold and len(critical_failures) == 0
        
        evaluation = ConversationEvaluation(
            conversation_id=f"conv_{len(conversation)}",
            scenario_name=scenario_description.split(':')[0] if ':' in scenario_description else scenario_description,
            total_score=total_score,
            max_possible_score=max_possible_score,
            success_percentage=success_percentage,
            criteria_results=criteria_results,
            overall_issues=all_issues,  # Pozostaw bez usuwania duplikatÃ³w - mogÄ… byÄ‡ sÅ‚owniki
            overall_suggestions=all_suggestions,  # Pozostaw bez usuwania duplikatÃ³w
            conversation_analysis=conversation_analysis,
            critical_failures=critical_failures,
            passes_quality_gate=passes_quality_gate
        )
        
        # Loguj wynik
        if passes_quality_gate:
            logger.success(f"âœ… OCENA POZYTYWNA: {success_percentage:.1f}% ({total_score:.1f}/{max_possible_score:.1f})")
        else:
            logger.error(f"âŒ OCENA NEGATYWNA: {success_percentage:.1f}% ({total_score:.1f}/{max_possible_score:.1f})")
            if critical_failures:
                logger.error(f"ğŸ’¥ KRYTYCZNE BÅÄ˜DY: {len(critical_failures)}")
        
        return evaluation
    
    async def _evaluate_single_criteria(
        self, 
        conversation: List[Dict[str, Any]], 
        scenario: str, 
        criteria: EvaluationCriteria,
        expected_behaviors: Optional[List[str]]
    ) -> EvaluationResult:
        """Ocenia konwersacjÄ™ wedÅ‚ug pojedynczego kryterium."""
        
        prompt = self._build_evaluation_prompt(conversation, scenario, criteria, expected_behaviors or [])
        
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": self.model_name,
                    "messages": [
                        {
                            "role": "system",
                            "content": "JesteÅ› BEZWZGLÄ˜DNYM oceniaczem AI asystentÃ³w. Twoja praca to znalezienie kaÅ¼dego bÅ‚Ä™du i niedociÄ…gniÄ™cia. BÄ…dÅº surowy jak najgorszy krytyk. Oceniaj jak profesor na egzaminie doktorskim - kaÅ¼dy bÅ‚Ä…d to znaczne obniÅ¼enie oceny. NIE BÄ„DÅ¹ ÅAGODNY!"
                        },
                        {
                            "role": "user", 
                            "content": prompt
                        }
                    ],
                    "temperature": 0.1,  # Niska temperatura dla konsystentnych ocen
                    "max_tokens": 2000
                }
                
                async with session.post(
                    f"{self.lm_studio_url}/v1/chat/completions",
                    json=payload,
                    headers={"Content-Type": "application/json"}
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        result_text = data["choices"][0]["message"]["content"]
                        return self._parse_evaluation_result(result_text, criteria)
                    else:
                        error_text = await response.text()
                        raise Exception(f"LM Studio error {response.status}: {error_text}")
            
        except Exception as e:
            logger.error(f"BÅ‚Ä…d oceny {criteria.value}: {e}")
            return EvaluationResult(
                criteria=criteria,
                score=0.0,
                max_score=10.0,
                reasoning=f"BÅ‚Ä…d podczas oceny: {e}",
                issues=[f"Nie udaÅ‚o siÄ™ oceniÄ‡ {criteria.value}"],
                suggestions=["SprawdÅº poÅ‚Ä…czenie z LM Studio"],
                severity="critical"
            )
    
    def _build_evaluation_prompt(
        self, 
        conversation: List[Dict[str, Any]], 
        scenario: str, 
        criteria: EvaluationCriteria,
        expected_behaviors: Optional[List[str]]
    ) -> str:
        """Buduje surowy prompt dla konkretnego kryterium oceny."""
        
        # Przygotuj konwersacjÄ™ w czytelnym formacie
        conversation_text = ""
        for i, turn in enumerate(conversation, 1):
            user_msg = turn.get('query', turn.get('message', 'BRAK WIADOMOÅšCI'))
            
            # WyciÄ…gnij odpowiedÅº AI z zagnieÅ¼dÅ¼onej struktury
            ai_response = "BRAK ODPOWIEDZI"
            if 'response' in turn:
                resp_data = turn['response']
                if isinstance(resp_data, dict):
                    if 'data' in resp_data and isinstance(resp_data['data'], dict):
                        ai_response = resp_data['data'].get('response', 'BRAK ODPOWIEDZI')
                    else:
                        ai_response = str(resp_data)
                else:
                    ai_response = str(resp_data)
            
            conversation_text += f"\n=== TURA {i} ===\nğŸ‘¤ UÅ»YTKOWNIK: {user_msg}\nğŸ¤– GAJA AI: {ai_response}\n"
        
        criteria_descriptions = {
            EvaluationCriteria.ACCURACY: """
            ğŸ¯ KRYTERIUM: DOKÅADNOÅšÄ† FAKTYCZNA [MAKSYMALNA SUROWOÅšÄ†]
            SPRAWDÅ¹ BEZWZGLÄ˜DNIE:
            âŒ Czy KAÅ»DA informacja jest w 100% poprawna faktycznie
            âŒ Czy daty, liczby, fakty sÄ… ABSOLUTNIE zgodne z rzeczywistoÅ›ciÄ…  
            âŒ Czy nie ma ANI JEDNEGO bÅ‚Ä™du merytorycznego
            âŒ Czy nie ma Å»ADNYCH halucynacji lub wymyÅ›lonych informacji
            âŒ Czy AI nie podaje informacji ktÃ³rych nie moÅ¼e wiedzieÄ‡
            
            OCENA: JeÅ›li znajdziesz JAKIKOLWIEK bÅ‚Ä…d faktyczny â†’ DRASTYCZNIE obniÅ¼ ocenÄ™!
            """,
            EvaluationCriteria.RELEVANCE: """
            ğŸ¯ KRYTERIUM: TRAFNOÅšÄ† ODPOWIEDZI [ZERO TOLERANCJI]
            SPRAWDÅ¹ BEZWZGLÄ˜DNIE:
            âŒ Czy KAÅ»DA odpowiedÅº BEZPOÅšREDNIO odpowiada na pytanie
            âŒ Czy AI NIE odchodzi od tematu ANI PRZEZ CHWILÄ˜
            âŒ Czy nie ma Å»ADNYCH niepotrzebnych informacji
            âŒ Czy KAÅ»DE sÅ‚owo sÅ‚uÅ¼y realizacji Å¼Ä…dania uÅ¼ytkownika
            âŒ Czy AI nie gada o rzeczach o ktÃ³re nie pytano
            
            OCENA: KaÅ¼de zejÅ›cie z tematu = DUÅ»E obniÅ¼enie oceny!
            """,
            EvaluationCriteria.COMPLETENESS: """
            ğŸ¯ KRYTERIUM: KOMPLETNOÅšÄ† [PERFEKCJONIZM WYMAGANY]
            SPRAWDÅ¹ BEZWZGLÄ˜DNIE:
            âŒ Czy odpowiedÅº realizuje WSZYSTKIE elementy Å¼Ä…dania
            âŒ Czy nie pominiÄ™to Å»ADNEGO waÅ¼nego elementu
            âŒ Czy wszystkie wymagane informacje sÄ… dostarczone
            âŒ Czy poziom szczegÃ³Å‚owoÅ›ci jest IDEALNY (nie za maÅ‚o, nie za duÅ¼o)
            âŒ Czy Å¼Ä…danie zostaÅ‚o zrealizowane w 100%
            
            OCENA: KaÅ¼dy brakujÄ…cy element = DRASTYCZNE obniÅ¼enie!
            """,
            EvaluationCriteria.TOOL_USAGE: """
            ğŸ¯ KRYTERIUM: UÅ»YWANIE NARZÄ˜DZI [MAKSYMALNA PRECYZJA]
            SPRAWDÅ¹ BEZWZGLÄ˜DNIE:
            âŒ Czy AI uÅ¼ywa DOKÅADNIE odpowiednich narzÄ™dzi gdy powinno
            âŒ Czy NIE uÅ¼ywa narzÄ™dzi gdy nie powinno (overhead)
            âŒ Czy wybiera NAJLEPSZE dostÄ™pne narzÄ™dzie
            âŒ Czy poprawnie interpretuje wyniki z narzÄ™dzi
            âŒ Czy informuje uÅ¼ytkownika o uÅ¼yciu narzÄ™dzi gdy trzeba
            
            OCENA: KaÅ¼de zÅ‚e uÅ¼ycie/brak uÅ¼ycia narzÄ™dzia = DUÅ»A kara!
            """,
            EvaluationCriteria.CONTEXT_UNDERSTANDING: """
            ğŸ¯ KRYTERIUM: ZROZUMIENIE KONTEKSTU [INTELIGENCJA WYMAGANA]
            SPRAWDÅ¹ BEZWZGLÄ˜DNIE:
            âŒ Czy AI rozumie PEÅNY kontekst kaÅ¼dej rozmowy
            âŒ Czy nawiÄ…zuje do wczeÅ›niejszych wypowiedzi PRECYZYJNIE
            âŒ Czy pamiÄ™ta WSZYSTKIE wczeÅ›niejsze ustalenia
            âŒ Czy interpretuje zamiary uÅ¼ytkownika TRAFNIE
            âŒ Czy buduje na kontekÅ›cie w sposÃ³b INTELIGENTNY
            
            OCENA: KaÅ¼da utrata kontekstu = POWAÅ»NE obniÅ¼enie!
            """,
            EvaluationCriteria.PROACTIVITY: """
            ğŸ¯ KRYTERIUM: PROAKTYWNOÅšÄ† [INICJATYWA WYMAGANA]  
            SPRAWDÅ¹ BEZWZGLÄ˜DNIE:
            âŒ Czy AI proponuje MÄ„DRE dodatkowe akcje
            âŒ Czy antycypuje potrzeby uÅ¼ytkownika TRAFNIE
            âŒ Czy sugeruje powiÄ…zane funkcjonalnoÅ›ci SENSOWNIE
            âŒ Czy dziaÅ‚a nie tylko reaktywnie ale PRZEWIDUJE
            âŒ Czy inicjatywa AI jest WARTOÅšCIOWA a nie irytujÄ…ca
            
            OCENA: Brak proaktywnoÅ›ci = Å›rednia ocena. ZÅ‚a proaktywnoÅ›Ä‡ = duÅ¼a kara!
            """,
            EvaluationCriteria.CONVERSATION_FLOW: """
            ğŸ¯ KRYTERIUM: PÅYNNOÅšÄ† KONWERSACJI [NATURALNOÅšÄ† WYMAGANA]
            SPRAWDÅ¹ BEZWZGLÄ˜DNIE:
            âŒ Czy rozmowa pÅ‚ynie ABSOLUTNIE naturalnie
            âŒ Czy odpowiedzi sÄ… spÃ³jne stylistycznie w 100%
            âŒ Czy ton jest PERFEKCYJNIE odpowiedni i konsekwentny
            âŒ Czy nie ma Å»ADNYCH nielogicznych przeskokÃ³w
            âŒ Czy AI brzmi jak kompetentny asystent a nie bot
            
            OCENA: KaÅ¼da nienaturalnoÅ›Ä‡ = obniÅ¼enie! Brzmi jak bot = duÅ¼a kara!
            """,
            EvaluationCriteria.ERROR_HANDLING: """
            ğŸ¯ KRYTERIUM: OBSÅUGA BÅÄ˜DÃ“W [PROFESJONALIZM WYMAGANY]
            SPRAWDÅ¹ BEZWZGLÄ˜DNIE:
            âŒ Czy AI obsÅ‚uguje bÅ‚Ä™dy w sposÃ³b GRACEFUL
            âŒ Czy informuje o problemach ZROZUMIALE dla uÅ¼ytkownika
            âŒ Czy prÃ³buje alternatywnych rozwiÄ…zaÅ„ MÄ„DRZE
            âŒ Czy NIE zostawia uÅ¼ytkownika bez pomocnej informacji
            âŒ Czy bÅ‚Ä™dy nie psujÄ… caÅ‚ego doÅ›wiadczenia
            
            OCENA: KaÅ¼dy Åºle obsÅ‚uÅ¼ony bÅ‚Ä…d = DUÅ»A kara!
            """,
            EvaluationCriteria.MEMORY_USAGE: """
            ğŸ¯ KRYTERIUM: UÅ»YWANIE PAMIÄ˜CI [INTELIGENCJA DÅUGOTERMINOWA]
            SPRAWDÅ¹ BEZWZGLÄ˜DNIE:
            âŒ Czy AI pamiÄ™ta wczeÅ›niejsze interakcje PRECYZYJNIE  
            âŒ Czy wykorzystuje informacje z poprzednich rozmÃ³w MÄ„DRZE
            âŒ Czy buduje na kontekÅ›cie dÅ‚ugoterminowym SENSOWNIE
            âŒ Czy personalizuje na bazie historii TRAFNIE
            âŒ Czy nie zapomina waÅ¼nych rzeczy w trakcie rozmowy
            
            OCENA: KaÅ¼da utrata pamiÄ™ci = POWAÅ»NE obniÅ¼enie!
            """,
            EvaluationCriteria.PERSONALIZATION: """
            ğŸ¯ KRYTERIUM: PERSONALIZACJA [INDYWIDUALNE PODEJÅšCIE]
            SPRAWDÅ¹ BEZWZGLÄ˜DNIE:
            âŒ Czy odpowiedzi sÄ… IDEALNIE dostosowane do uÅ¼ytkownika
            âŒ Czy uwzglÄ™dnia preferencje uÅ¼ytkownika DOKÅADNIE
            âŒ Czy ton i styl sÄ… PERFEKCYJNIE odpowiednie
            âŒ Czy uÅ¼ywa informacji o uÅ¼ytkowniku SENSOWNIE
            âŒ Czy personalizacja NIE jest natrÄ™tna lub niewÅ‚aÅ›ciwa
            
            OCENA: Brak personalizacji lub zÅ‚a personalizacja = obniÅ¼enie!
            """
        }
        
        return f"""
ğŸ”¥ MISJA: BEZWZGLÄ˜DNA OCENA ASYSTENTA AI ğŸ”¥

SCENARIUSZ TESTOWY: {scenario}

OCZEKIWANE ZACHOWANIA (te MUSZÄ„ byÄ‡ speÅ‚nione):
{chr(10).join(f"âœ… {behavior}" for behavior in (expected_behaviors or []))}

{criteria_descriptions[criteria]}

KONWERSACJA DO SUROWEJ OCENY:
{conversation_text}

ğŸ¯ ZADANIE OCENIAJÄ„CE:
OceÅ„ powyÅ¼szÄ… konwersacjÄ™ BEZWZGLÄ˜DNIE wedÅ‚ug kryterium {criteria.value}. 
BÄ…dÅº MAKSYMALNIE surowy - to nie jest ocena koleÅ¼eÅ„ska ale profesjonalna!
ZnajdÅº KAÅ»DY bÅ‚Ä…d, kaÅ¼de niedociÄ…gniÄ™cie, kaÅ¼dÄ… sÅ‚aboÅ›Ä‡!

ZASADY OCENIANIA:
â€¢ 0-2 pkt = KATASTROFA (caÅ‚kowite niepowodzenie)
â€¢ 3-4 pkt = BARDZO ZÅE (wiÄ™kszoÅ›Ä‡ nie dziaÅ‚a)
â€¢ 5-6 pkt = ZÅE (kilka rzeczy dziaÅ‚a, ale duÅ¼o problemÃ³w)
â€¢ 7-8 pkt = PRZECIÄ˜TNE (w porzÄ…dku ale widoczne problemy)  
â€¢ 9-10 pkt = DOSKONAÅE (prawie/caÅ‚kowicie bezbÅ‚Ä™dne)

NIE BÄ„DÅ¹ ÅAGODNY! KaÅ¼dy bÅ‚Ä…d to obniÅ¼enie!

ODPOWIEDZ DOKÅADNIE W TYM FORMACIE JSON:
{{
    "score": X.X,
    "max_score": 10.0,
    "reasoning": "BARDZO szczegÃ³Å‚owe uzasadnienie - dlaczego taka ocena, co dokÅ‚adnie nie gra, co jest dobre a co zÅ‚e",
    "issues": ["Lista KONKRETNYCH problemÃ³w jakie znalazÅ‚eÅ›", "KaÅ¼dy problem opisz dokÅ‚adnie"],
    "suggestions": ["Lista KONKRETNYCH sugestii jak to naprawiÄ‡", "KaÅ¼da sugestia konkretna i wykonalna"],
    "severity": "low/medium/high/critical"
}}

UWAGA: JeÅ›li znajdziesz POWAÅ»NE bÅ‚Ä™dy â†’ severity = "critical" i ocena musi byÄ‡ drastycznie niska!
"""
    
    def _parse_evaluation_result(self, result_text: str, criteria: EvaluationCriteria) -> EvaluationResult:
        """Parsuje odpowiedÅº modelu do struktury EvaluationResult."""
        try:
            # ZnajdÅº JSON w odpowiedzi (moÅ¼e byÄ‡ otoczony tekstem)
            json_start = result_text.find('{')
            json_end = result_text.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = result_text[json_start:json_end]
                
                # SprawdÅº czy JSON jest poprawny, jeÅ›li nie - sprÃ³buj naprawiÄ‡
                try:
                    data = json.loads(json_str)
                except json.JSONDecodeError:
                    # SprÃ³buj wyciÄ…gnÄ…Ä‡ tylko pierwszÄ… strukturÄ™ JSON
                    bracket_count = 0
                    end_pos = json_start
                    for i, char in enumerate(json_str):
                        if char == '{':
                            bracket_count += 1
                        elif char == '}':
                            bracket_count -= 1
                            if bracket_count == 0:
                                end_pos = json_start + i + 1
                                break
                    
                    json_str = result_text[json_start:end_pos]
                    data = json.loads(json_str)
                
                return EvaluationResult(
                    criteria=criteria,
                    score=float(data.get('score', 0.0)),
                    max_score=float(data.get('max_score', 10.0)),
                    reasoning=data.get('reasoning', 'Brak uzasadnienia'),
                    issues=data.get('issues', []),
                    suggestions=data.get('suggestions', []),
                    severity=data.get('severity', 'medium')
                )
            else:
                raise ValueError("Nie znaleziono JSON w odpowiedzi")
                
        except Exception as e:
            logger.error(f"BÅ‚Ä…d parsowania wyniku dla {criteria.value}: {e}")
            logger.debug(f"Surowa odpowiedÅº: {result_text}")
            
            return EvaluationResult(
                criteria=criteria,
                score=0.0,
                max_score=10.0,
                reasoning=f"BÅ‚Ä…d parsowania odpowiedzi: {e}",
                issues=[f"Model nie odpowiedziaÅ‚ w poprawnym formacie"],
                suggestions=["SprawdÅº poprawnoÅ›Ä‡ prompta i modelu"],
                severity="critical"
            )
    
    async def _analyze_conversation_flow(self, conversation: List[Dict[str, Any]], scenario: str) -> str:
        """Analizuje ogÃ³lny przebieg konwersacji z maksymalnÄ… surowoÅ›ciÄ…."""
        
        conversation_text = ""
        for i, turn in enumerate(conversation, 1):
            user_msg = turn.get('query', turn.get('message', 'BRAK'))
            
            ai_response = "BRAK ODPOWIEDZI"
            if 'response' in turn:
                resp_data = turn['response']
                if isinstance(resp_data, dict) and 'data' in resp_data:
                    ai_response = resp_data['data'].get('response', 'BRAK ODPOWIEDZI')
                else:
                    ai_response = str(resp_data)
            
            conversation_text += f"\nTURA {i}:\nğŸ‘¤ {user_msg}\nğŸ¤– {ai_response}\n"
        
        prompt = f"""
ğŸ”¥ BEZWZGLÄ˜DNA ANALIZA CAÅEJ KONWERSACJI ğŸ”¥

SCENARIUSZ: {scenario}

KONWERSACJA:
{conversation_text}

ğŸ¯ ZADANIE:
Przeanalizuj caÅ‚Ä… konwersacjÄ™ jako CIÄ„GÅY PRZEPÅYW z maksymalnÄ… surowoÅ›ciÄ….
BÄ…dÅº jak najgorszy krytyk - znajdÅº kaÅ¼dy problem!

SKONCENTRUJ SIÄ˜ NA:
1. Czy konwersacja FAKTYCZNIE realizuje zaÅ‚oÅ¼enia scenariusza? 
2. Jakie sÄ… WSZYSTKIE problemy w przepÅ‚ywie rozmowy?
3. Czy AI brzmi kompetentnie czy jak niedouczony bot?
4. Czy logika konwersacji jest spÃ³jna?
5. OgÃ³lny poziom profesjonalizmu (bÄ…dÅº surowy!)

Napisz 3-4 zdania SUROWEJ analizy. Nie oszczÄ™dzaj krytyki!
JeÅ›li coÅ› jest Åºle - powiedz to wprost i bezwzglÄ™dnie!
"""
        
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": self.model_name,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.1,
                    "max_tokens": 1000
                }
                
                async with session.post(
                    f"{self.lm_studio_url}/v1/chat/completions",
                    json=payload,
                    headers={"Content-Type": "application/json"}
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data["choices"][0]["message"]["content"]
                    else:
                        return f"BÅ‚Ä…d analizy: HTTP {response.status}"
        except Exception as e:
            return f"BÅ‚Ä…d analizy konwersacji: {e}"
